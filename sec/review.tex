\section{Literature review}\label{sec:review}

This section comprises what could be considered a classical literature review:
an analysis of selected articles and publications that constitute the state and
progress of each research topic.


\subsection{Clustering}\label{subsec:clustering}

Clustering, or \emph{cluster analysis}, is a generic term used to describe a
number of techniques whose objective is to partition some dataset into parts
(clusters) according to some distance (similarity) measure. The generated
partition should be such that the members of one cluster are more similar to one
another than the rest of the data~\cite{Everitt2011}. This form of analysis has
found applications in a multitude of fields, including recently the optimisation
of energy systems~\cite{Jing2019,Teichgraeber2019}, wireless sensor
networking~\cite{Goswami2019}, and the biological
sciences~\cite{Bulut2020,Kiselev2019}. Often lumped in with the task of
classification, clustering is distinct in that it reveals the underlying
structure of a dataset with knowledge that is fixed from the outset exclusively.
The alternative being a method with access to some ground truth to refer to,
such as a labelling.

Clustering originated in the social sciences around the turn of the
20\textsuperscript{th} century in a similar manner to several other (now
ubiquitous) statistical tools, including hypothesis testing and \(p\)-values,
correlation, and factor analysis. Early work on cluster analysis, such as Driver
and Kroeber~\cite{Driver1932} and Cattell~\cite{Cattell1943}, focused on its
applications in attempting to identify traits which led to cultural and
psychological differences between groups of people, as opposed to studying the
methods used to identify any partitions.

The focus of clustering research shifted to the statistical nature of the
methods following work by Sokal~\cite{Sokal1966} (later revised with
Sneath~\cite{Sneath1973}) on clustering under the name \emph{numerical
taxonomy}. These works were highly influential and led to an abundance of
research into clustering methods, including~\cite{Diday1976}
and~\cite{Hartigan1975} which each formalised the principles of clustering as a
part of statistics. Furthermore, this period witnessed the advent of seminal
work on clustering algorithms such as \(k\)-means~\cite{Hartigan1979} and
hierarchical clustering~\cite{Defays1977,Sibson1973} that are still considered
fundamental methods.

In tandem with this shift was the rise in the use of software to implement and
compute algorithms (including clustering), and so electronic data became
essential. As the size and complexity of research data increased, so did the
span of the field that is now referred to as \emph{machine learning}. Machine
learning is a loose term to describe how a computer (machine) may `learn' from a
potentially large source of data without following explicit instructions. Owing
in part to this vague definition, machine learning comprises a great deal of
techniques that were borne out of statistics, including regression,
classification and clustering, despite having lost its connection with the
statistics that underpin it, at least colloquially. In particular, clustering
falls comfortably into the category of \emph{unsupervised learning} techniques
as a method that relies solely on observed data and some a priori
knowledge such as a set of parameters~\cite{Dayan1999}.

The remainder of this section offers a summary of the literature around the
three principle types of clustering algorithm: those belonging to the
\(k\)-means paradigm, hierarchical connectivity models, and, finally,
density-based algorithms. In addition to these, fuzzy clustering and model-based
clustering are popular. For overviews and reviews of the methods belonging to
the former, consider~\cite{Ferraro2019,Gosain2016,Li2016},
and~\cite{Bouveyron2019,Fruhwirth2019,McNicholas2016} for the latter.

\subsubsection{The \(k\)-means paradigm}\label{subsubsec:kmeans}

The concept of \(k\)-means clustering is straightforward: partition a
real-valued dataset into \(k \in \mathbb N\) homogeneous parts (clusters), where
each data point is assigned to the cluster to which the point is closest. The
sum of these within-cluster distances is often referred to as the \emph{inertia}
of the clustering. The distance from a point to a cluster is defined as the
distance from the point to the mean of the elements in that cluster, also
referred to as the cluster centre or centroid; this alternative name gives
another name to the paradigm, \emph{centroid-based clustering}. Typically, the
distance measure used is the Euclidean distance, with the assumption that
several preprocessing techniques are used to mitigate any scaling discrepancies
in the attributes of the data.

Although \(k\)-means clustering has independently been discovered a number of
times since as early as the 1950s (comprehensive histories on the subject can be
found in~\cite{Bock2007,Jain2010}), its formulation is commonly attributed
to~\cite{Hartigan1979}. Given its lengthy standing, there are a multitude of
algorithms that perform \(k\)-means clustering. However, the most commonly used
is Lloyd's algorithm~\cite{Lloyd1982}. This procedure is remarkably
uncomplicated in its statement, and has now become so synonymous with
\(k\)-means clustering that it is referred to as `the \(k\)-means algorithm'. To
summarise it in a sentence, the algorithm iteratively partitions the numerical
data into \(k\) parts, reassigning data points and adjusting cluster means until
no point moves cluster on a full cycle of the dataset. The algorithm has proven
popular due to this simplicity. Furthermore, it is scalable and can be
parallelised~\cite{Bahmani2012}, and its implementations are
concise~\cite{Olafsson2008,Wu2009}.

However, centroid-based clustering is not without its drawbacks. The most
notable of these is its dependency on being presented with data that can be
naturally partitioned into \(k\) clusters of a particular
type~\cite{Ostrovsky2013}. Namely, the true clusters should be isotropic, convex
and linearly separable. The reason these conditions exist, particularly for
\(k\)-means, is because the \(k\)-means (Lloyd's) algorithm is a heuristic for
identifying the centroidal Voronoi tessellation of a dataset~\cite{Du2006}.
Hence, the true objective of the algorithm is to divide the attribute space into
\(k\) equally weighted parts using the dataset as a sample, rather than focusing
on dividing the dataset itself.

Regardless of the true nature of a centroid-based clustering algorithm, there is
the issue of choosing a suitable value of \(k\) prior to clustering. A common
approach to this problem is the so-called `elbow' method where a clustering
algorithm is run using a range of values for \(k\); this range is typically
informed by what is considered reasonable in the problem domain. From these
runs, the value of \(k\) is plotted against some objective and the plot is
inspected for a kink or `elbow'. Typically, inertia is used, but another popular
metric is the silhouette coefficient: a measure for both the intra-cluster
tightness and inter-cluster separation of a clustering~\cite{Rousseeuw1987}.

Irrespective of the objective used, the definition of what constitutes an elbow
is not necessarily well-defined.~\cite{Sujatha2013,Syakur2018} each employ the
elbow method with final inertia as the objective. In each case, the authors
offer little explanation of their methodologies other than choosing \(k\) where
`drastic' changes occur in the objective. Attempts to formalise this method have
been made, however. For instance,~\cite{Satopaa2011} provides a `knee-point'
detection algorithm that finds the point of maximal curvature in the domain of a
function when provided with some basic properties of that function (whether it
is convex, monotonic, etc.). This algorithm has been adapted to cleanly carry
out the elbow method in software packages, such as
Yellowbrick~\cite{Bengfort2019}, by interpolating the objective values.

Further to this formalisation of the elbow method, other extensions to
\(k\)-means itself have been presented to alleviate all dependency on making a
choice for \(k\).~\cite{Olukanmi2019} proposes a automatic decision scheme for
\(k\)-means based on a vector comprised of the values that constitute the
inertia of a clustering. The process determines that if this vector meets some
criterion attached to its standard deviation, then each cluster is sufficiently
unimodal and Gaussian, fulfilling the condition that clusters be roughly
spherical.

Another limitation of \(k\)-means is that it is dependent on its initial
solution. The standard initialisation process is to select \(k\) points in the
dataset at random, introducing a stochastic element to the algorithm. Studying
methods to find sensible (ideally, deterministic) initial solutions has formed a
substantial part of centroid-based clustering research. For
instance,~\cite{Manoharan2016} provides a novel initialisation that pursues the
Voronoi tessellation by use of a `divide-and-conquer' approach. Under the
presented scheme, the initial centroids are selected based on a rough partition
of the attribute space according to their extreme values.
Likewise,~\cite{Singh2013} offers an initialisation that assigns each centroid
using the arithmetic mean and dividing the remaining attribute space in two.
Meanwhile,~\cite{Katara2015} initialises \(k\)-means by sorting the instances in
the dataset according to their distance from the origin. Once sorted, the
instances are split into \(k\) disjoint sets and the mean of each set is
assigned as a centroid. A likely issue with this approach is that there are new
assumptions about the data, i.e. that it is centred about the origin in all
dimensions (rectifiable through feature scaling), and that all points
equidistant from the origin are similar to one another.

A crucial part of centroid-based clustering research is dedicated to alternative
measurements for the centre of a cluster. A cluster centroid is broadly regarded
as a purely geometric object, and is measured using a distance metric and a
measure of central tendency. Using the Euclidean mean as the centre of a cluster
is a sensible choice when clustering continuous, normalised data. However, if
the data is not scalable (if it is categorical, for instance) then this approach
is not suitable. Further, the Euclidean distance is linear, leading \(k\)-means
to be prone to the effects of outliers in the data. As such, numerous
extensions to Lloyd's algorithm exist in the literature that are designed to be
more robust to these limitations. For continuous (or at least ordered) data,
\(k\)-medians and \(k\)-medoids exist. The former uses the median value of each
attribute in a cluster to form the centre~\cite{Arya2001,Bradley1997},
mitigating the effect of outliers.

In \(k\)-medoids, the centre of a cluster is taken to be the data point
which is closest to all other points in the cluster~\cite{Kaufman1987}.
Typically, \(k\)-medoids makes use of the Manhattan distance, but is generic in
its schema. If Euclidean distance is used, then \(k\)-medoids is identical to
\(k\)-means, with the restriction that the centre of the cluster is an actual
point in that cluster. Specifying that the centre be a real point is seen as the
primary benefit of \(k\)-medoids, and continues to be actively
studied~\cite{Schubert2019,Ushakov2021}. There are cases where a virtual cluster
centre is not sufficient, such as facility allocation~\cite{Chen2016,Wang2020}
and the clustering of gene sequences~\cite{Johnson2018}.

When considering a categorical or mixed-type dataset, none of the aforementioned
clustering techniques will work as they were intended because the sense of order
does not exist across the entire attribute space. While it is possible to use
\(k\)-means in these situations, by converting any categorical attributes into
pseudo-numerical (binary) attributes by use of dummy coding, this is not
recommended. One potential failing is the inflated effect of categorical
variables with many distinct values in any distance calculation, as these will
produce more binary variables. Another is that by introducing potentially many
binary variables, the dimensionality of the dataset increases substantially, and
so the points in the higher-dimension form of the data become intrinsically
further from one another, thus loosening any sense of cluster homogeneity.

Therefore, another approach should be taken that handles the data directly. The
\(k\)-modes and \(k\)-prototypes algorithms (introduced in~\cite{Huang1998}) are
capable of handling categorical and mixed-type data, respectively. The
\(k\)-modes algorithm is an extension to Lloyd's algorithm, like \(k\)-medians
and \(k\)-medoids, where the cluster centre is defined as a point whose
attribute values are most frequent among those of the points in a
cluster.~\cite{Huang1998} provides a so-called `matching dissimilarity' measure
to expedite the centre calculations, making \(k\)-modes scalable and
computationally efficient~\cite{Madhuri2014}. The \(k\)-prototypes algorithm is
essentially a blended form of \(k\)-modes and \(k\)-means where each method is
applied to the categorical and numeric attributes at each iteration, before
their respective costs are combined linearly according to some parameter,
\(\gamma\). Work into \(k\)-modes clustering is largely focused on the initial
choice of centroids, as
in~\cite{Cao2009,Jiang2016,Khan2013,Khan2007,Taoying2013}, and
improving on the original distance measure.~\cite{Cao2012} and~\cite{Ng2007}
offer two popular, novel dissimilarity measures for \(k\)-modes clustering that
improve on the basic measure by taking into account the relative frequency of
the values of each attribute.~\cite{Ng2007} focuses locally to the cluster at
hand, while~\cite{Cao2012} considers their frequencies from a universal
perspective.

While centroid-based clustering is dominated by Lloyd-like algorithms, recent
work has sought out alternative approached. For instance,~\cite{Chen2019}
revises centroid clustering from an expectation-maximisation process to one that
considers its data points as agents that act with preferences based on their
locale. The overall aim of this approach is to incorporate a sense of
mathematical fairness by identifying a solution that can not be justifiably
rejected by any subset of its agents. Such a solution is found through a
proportional (i.e. Pareto-optimal) distribution of the cluster centres according
to some distance metric.

At this point, the survey returns to the effect of outliers on Lloyd-like
algorithms, especially \(k\)-means, and the literature around reducing that
effect.~\cite{Olukanmi2017} introduces the \(k\)-means\# algorithm --- an
extension to \(k\)-means that embeds an outlier detection feature which relies
solely on the underlying structures from the original \(k\)-means algorithm.
The benefit of handling adverse noise in this way is twofold: first, it
preserves the computational efficiencies that come with using the mean (as
opposed to sorting the attributes of each cluster to find a median in
\(k\)-medians, say); and, second, there are no additional parameters to
determine.~\cite{Gupta2017} provides another integration with \(k\)-means for
detecting outliers by way of a local search, but in order to do so, it
introduces another parameter determining the cardinality of the neighbourhood of
a point. In contract to these extensions,~\cite{Lei2012,Wei2019} are examples of
works that exploit this weakness of \(k\)-means clustering. In each case, the
\(k\)-means algorithm partitions a dataset, and outliers are identified
according to some neighbourhood threshold based on the density of the cluster.

The final remark in this section of the survey is concerned with the relaxation
of the constraints on cluster geometry by centroid-based clustering algorithms.
This particularity is fundamental to how these algorithms operate and are
designed, and so little research has been done on the subject. Having said
that,~\cite{Sung1998} demonstrates how the Mahalanobis distance can be used in
place of the Euclidean distance to circumvent the dependency on spherical (i.e.
isotropic) clusters by \(k\)-means. However, the resultant clusters must still
be convex and linearly separable. The Mahalanobis distance is commonly used to
detect outliers in other settings, and is dependent on the eigenvalues of the
covariance matrix derived from that dataset~\cite{Mahalanobis1936}; a potential
downside to this measure is the singularity of that matrix, which is
particularly likely in high-dimensional data. Another exemplar work
is~\cite{Statman2020} where a variant of \(k\)-means is presented as being
agnostic to its distance measure, and removes the requirement for the attribute
space to be a metric space at all.

\subsubsection{Hierarchical clustering}

Hierarchical clustering seeks to identify a hierarchy of the relationships
between subsets of points in a dataset. A hierarchical clustering algorithm
depends on a distance metric for measuring inter-point similarity, and a
\emph{linkage criterion} to determine the distance between two sets of points.
For the most part, algorithms for hierarchical clustering are one of two types:
agglomerative or divisive~\cite{Kaufman1990}. Agglomerative algorithms,
originating with~\cite{Johnson1967,Ward1963}, begin with each point in its own
cluster and work to merge clusters together, constructing the hierarchy from the
bottom up. Meanwhile, divisive algorithms, with~\cite{Edwards1965} being an
early example, take a top-down approach and seek to split the dataset into
reasonably homogeneous clusters.

Since its origins, hierarchical clustering research has been dominated by
agglomerative methods; this is due, largely, to the computational issues
associated with how to best split a heterogeneous dataset into parts
recursively. For a dataset of \(n\) points, a complete search of the possible
splits takes \(2^n\) calculations, and so divisive methods require efficient
heuristics to be even remotely plausible on real-world data. A common choice is
the \(k\)-means algorithm~\cite{Moseley2017,Peterson2018}.

In addition, hierarchical clustering has been successfully reframed as a
combinatorial optimisation problem, as is done with divisive clustering
in~\cite{Dasgupta2016}, which allows for the application of optimisation
heuristics such as linear programming~\cite{Roy2017}, local
search~\cite{Aljarah2019} and probabilistic estimation~\cite{Fan2015} to
identify a clustering in reasonable time. Furthermore, related works exist which
concern themselves with determining the quality of a hierarchical structure with
respect to some cost function,
like~\cite{Bilu2012,Lyzinski2017}.~\cite{CohenAddad2018} combines these
methodologies to produce a divisive clustering algorithm that achieves
logarithmic complexity.

While heuristic methods exist for agglomerative
clustering~\cite{Aljarah2019,Fan2015}, its methods do not bear the same burdens
as divisive methods; the grouping together of parts can be achieved with more
straightforward tools. Generally, the definition of an agglomerative
algorithm is reliant on their linkage criterion. Meanwhile, the
distance metric used is largely dependent on the type of data
presented~\cite{Nielsen2016}. Two of the fundamental criteria for hierarchical
clustering are complete linkage, which defines the CLINK
algorithm~\cite{Defays1977}, and single linkage, providing the SLINK
algorithm~\cite{Sibson1973}. For two clusters, \(X\) and \(Y\), and a point-wise
distance metric, \(d\), each criterion, \(D\), is evaluated as follows:

\begin{itemize}
    \item Complete linkage: \(D(X, Y) = \max_{x \in X, y \in Y} d(x, y)\)
    \item Single linkage: \(D(X, Y) = \min_{x \in X, y \in Y} d(x, y)\)
\end{itemize}

These definitions give each approach the colloquial names \emph{farthest
neighbour} and \emph{nearest neighbour} clustering, respectively. Moreover,
defining linkage in this way allows a hierarchical (agglomerative or divisive)
algorithm to be run using only a point-wise distance matrix, as opposed to a
dataset directly, allowing for increases in computational efficiency by use of
caching~\cite{Nielsen2016}. However, hierarchical methods remain prone to
outliers and are biased towards globular clusters because of definitions such as
these. Ongoing reviews on the subject of hierarchical (although predominantly
agglomerative) clustering methods can be found by Murtagh~\cite{Murtagh1983} and
later with Contreras~\cite{Murtagh2012,Murtagh2017}.

Like centroid-based clustering, a principle issue with hierarchical clustering
is knowing how many clusters is the most appropriate number. Hierarchical
algorithms terminate when all points exist in a single cluster (for
agglomerative methods) or are all separate (for divisive). A key advantage of
clustering in this way is the retention of a complete history of how the points
relate to one another. These histories are used to visualise the clustering
process via a dendogram. A dendogram is an acyclic graph (tree) arranged into
levels corresponding to the stages at which two clusters are merged (or split).

With such a visualisation, and the underlying tree itself, an appropriate number
of clusters can be identified after running the algorithm once, as opposed to
the ranges of runs required for the elbow method in centroid-based
clustering.~\cite{Tellaroli2016} presents a hierarchical clustering schema that
incorporates complete-linkage clustering with the minimum variance criterion
presented in~\cite{Ward1963}. In doing so, the method automates the process of
choosing an appropriate number of clusters; a decision that is usually made post
hoc according to another metric, such as the silhouette coefficient, evaluated
at each level of the tree.

\subsubsection{Density-based clustering}

Density-based clustering differs from centroid-based methods in that
it actively seeks more of the underlying structure of a dataset. In
centroid-based clustering, decisions are made using the distance between two
points, which provides a \emph{flat} clustering without an underlying structure.
Meanwhile, hierarchical methods make use of identification and aggregation
devices, which are derived from point-wise distances, to measure and organise
the connectivity of subsets in a dataset, providing insight into the structure
of the data. Arguably, density-based clustering takes this structural approach a
step further and considers the attribute space of a dataset as a density
surface. Then, a cluster is defined as a high-density region of that surface.

By considering a dataset in this way, the clusters can be of arbitrary shape,
overcoming the major shortfall of the other paradigms considered here.
Furthermore, recognising data points in sparse areas of the surface (i.e. with
low density) affords density-based methods the automatic filtration of noise,
without having to implement a specific schema.

Density-based clustering is relatively young when compared to other paradigms,
only emerging at the end of the 20\textsuperscript{th} century with the DBSCAN
algorithm~\cite{Ester1996}. Given its youth, contemporary clustering literature
surveys (such as~\cite{Jain1999,Xu2005}) lacked thorough study of density-based
clustering methods. Over time, specific surveys have been published, culminating
in a most recent survey~\cite{Bhattacharjee2020} which provides an exceptionally
detailed review of density-based clustering algorithms. This survey includes a
descriptive taxonomy of 32 density-based methods, categorised by their density
definition, parameter sensitivity, mode of execution and the nature of the data
being clustered.

DBSCAN is a point-based, parameter-sensitive clustering algorithm that has
proven popular, and is now synonymous with density-based clustering. The initial
work has spurred a slue of extensions to DBSCAN which aim to bypass its
shortcomings, including Generalised DBSCAN~\cite{Sander1998}, Incremental
DBSCAN~\cite{Bakr2015,Ester1998}, Improved DBSCAN~\cite{Borah2004}, and various
parallelised versions of the algorithm~\cite{Bohm2009,He2011,Loh2014,Xu2002}.
The primary issues with DBSCAN relate to its parameters: a radius, \(\epsilon >
0\), defining the neighbourhood of a point, and an integer, \(Minpts \in \mathbb
N\), that specifies the minimum cardinality of a neighbourhood required for a
point to be considered dense. These parameters correspond to DBSCAN being
point-based: regions of high density are determined using the points directly.
The first disadvantage of DBSCAN is in estimating these
parameters.~\cite{Ester1996} provides some default values based on
dimensionality, but a good understanding of the dataset at hand is required for
effective use. This issue leads to the second, which is that the parameters are
global in scope, meaning that DBSCAN is unable to identify clusters that are of
varying densities. Finally, the performance of DBSCAN is limited in
high-dimensional space, because of the so-called \emph{curse of
dimensionality}~\cite{Keogh2017} --- the same is true of other clustering
algorithms, including \(k\)-means.

However, these limitations are not without an alternative. For instance,
CLIQUE~\cite{Agrawal1998} and OPTIGRID~\cite{Hinneburg1999} each provide
density-based clustering algorithms that are designed to perform well in
high-dimensional space. Each of these algorithms is grid-based (as opposed to
point-based), so the attribute space is discretised into rectangles. These
rectangles are then used to identify dense regions. To identify clusters of
variable densities, DVBSCAN~\cite{Ram2010} and HDBSCAN~\cite{Campello2013} are
available. The latter aggregates outputs from DBSCAN using a range of radii,
making it more resilient to changes in either parameter. The former runs DBSCAN
with the addition of allowing a cluster to expand provided its core points local
(\(\epsilon\)-neighbourhood) density satisfies some criterion; because of this,
it requires careful consideration of its parameters to perform well. Lastly, any
parameter-adaptive, density-based algorithm will do away with the trouble of
choosing precise parameters. Examples include the popular OPTICS
algorithm~\cite{Ankerst1999}, DBCLASD~\cite{Xu1998} for spatial data, and
DSets-DBSCAN~\cite{Hou2016}. The last provides a clustering that is independent
of the parameters provided, but is exclusively for the clustering of images. 

\subsection{Healthcare modelling}

Healthcare modelling is a broad term that encompasses a plethora of techniques
from a number of disciplines such as financial modelling and forecasting, and
operational problems like vehicle routing or staff rostering. This review
focuses on these operational pursuits, and particularly those that are concerned
with patients directly. The decision to narrow the literature in this way is
both practical and conscientious. Practical by reducing the span of literature
on `healthcare modelling' to something less cumbersome, and conscientious as it
allows this review to make a small contribution to the research wing of the
progressively more commonplace concept of patient-centred care.

This form of healthcare, formally defined in~\cite{Robinson2008}, demands that
the perspective of a healthcare system should align itself with its patients'
needs and lived experiences. The alternative to patient-centred care would be a
system in which patients are treated in an exact but vague way, according to
only the needs of the system, say. Some form of patient-centred care has been
adopted in healthcare systems around the world including both state-funded and
private systems~\cite{DoH2010,Dewi2013,Luxford2011}. Unsurprisingly, its
application has been commended by patients, advocates and practitioners alike
for improving condition-specific
populations~\cite{Foster2019,Gambling2010,Gondek2016,Tsianakas2012} and more
broadly~\cite{IAPO2012,Richards2015,Santana2019}.

\subsubsection{Segmentation analysis}

Segmentation analysis allows for the targeted analysis of otherwise
heterogeneous datasets and encompasses several techniques from operational
research, statistics and machine learning. One of the most desirable qualities
of this kind of analysis is the ability to glean and communicate simplified
summaries of patient needs to stakeholders within a healthcare
system~\cite{Vuik2016b, Yoon2020}. For instance, clinical profiling often forms
part of the broader analysis where each segment is summarised in a phrase or
infographic~\cite{Vuik2016a,Yan2019}.

The survey identified three commonplace groups of patient characteristics used
to segment a patient population: system utilisation metrics; clinical
attributes; and the pathway. The last is not used to segment the patients
directly, but instead groups patients' movements through a healthcare system;
this is typically done using a technique known as process
mining.~\cite{Arnolds2018}~and~\cite{Delias2015} demonstrate how this technique
can be used to improve the efficiency of a hospital system as opposed to
tackling the more relevant issue of patient-centred care. The remaining
characteristics can be segmented in a variety of ways, but recent works tend to
favour unsupervised methods --- typically latent class analysis (LCA) or
clustering~\cite{Yan2018}.

LCA is a statistical, model-based method used to identify groups (called latent
classes) in data by relating its observations to some unobserved (latent),
categorical attribute. This attribute has multiple possible categories, each
corresponding to a latent class. The discovered relations enable the
observations to be separated into latent classes according to their maximum
likelihood class membership~\cite{Hagenaars2002,Lazarsfeld1968}. This method has
proved useful in the study of comorbidity patterns as
in~\cite{Kuwornu2014,Larsen2017} where combinations of demographic and clinical
attributes are related to various subgroups of chronic diseases.

Similarly to LCA, clustering identifies groups (clusters) in data to produce
labels for its instances. However, as discussed in
Section~\ref{subsec:clustering}, clustering includes a wide variety of methods
where the common theme is to maximise homogeneity within, and heterogeneity
between, each cluster. Of those methods, \(k\)-means clustering is perhaps the
most widely used in healthcare; this is likely due to its simplicity and
scalability. In addition to \(k\)-means, hierarchical clustering methods can be
useful if a suitable number of parts cannot be found initially~\cite{Vuik2016a}.
Moreover, supervised hierarchical segmentation methods such as classification
and regression trees (as in~\cite{Harper2006}) have been used where an existing,
well-defined, label is of particular significance.

\subsubsection{Queuing models}

Since the seminal works by Erlang~\cite{Erlang1917,Erlang1920} established the
core concepts of queuing theory, the application of queues and queuing networks
to real services has become abundant, including the healthcare service. By
applying these models to healthcare settings, many aspects of the underlying
system can be studied. A common area of study in healthcare settings is of
service capacity.~\cite{McClain1976} is an early example of such work where
acute bed capacity was determined using hospital occupancy data. Meanwhile, more
modern works such as~\cite{Palvannan2012,Pinto2014} consider more extensive
sources of data to build their queuing models.  Moreover, the output of a model
is catered more towards being actionable --- as is the prerogative of
operational research. For instance,~\cite{Pinto2014} devises new categorisations
for both hospital beds and arrivals that are informed by the queuing model. A
further example is~\cite{Komashie2015} where queuing models are used to measure
and understand satisfaction among patients and staff.

In addition to these theoretic models, healthcare queuing research has expanded
to include computer simulation models. The simulation of queues, or networks
thereof, have the benefit of adeptly capturing the stochastic nuances of
hospital systems over their theoretic counterparts. Example areas include the
construction and simulation of Markov processes via process
mining~\cite{Arnolds2018,Rebuge2012}, and patient flow~\cite{Bhattacharjee2014}.
Regardless of the advantages of simulation models, a prerequisite is reliable
software with which to construct those simulations. A common approach to
building simulation models of queues is to use a graphical user interface such
as Simul8. These tools have the benefits of being highly visual, making them
attractive to organisations looking to implement queuing models without
necessary technical expertise, including the NHS.~\cite{Brailsford2013}
discusses the issues around operational research and simulation being taken up
in the NHS despite the availability of intuitive software packages like Simul8.
However, they do not address a core principle of good simulation work:
reproducibility. The ability to reliably reproduce a set of results is of great
importance to scientific research but remains an issue in simulation research
generally~\cite{Fitzpatrick2019}. When considering issues with reproducibility
in scientific computing (simulation included), the source of any concerns is
often with the software used~\cite{Ivie2018}. Using well-developed, open-source
software can alleviate issues around reproducibility and reliability as how they
are used involve less uncertainty and require more rigour than ‘drag-and-drop’
software. One example of such a piece of software is Ciw~\cite{Palmer2019}. Ciw
is a discrete event simulation library written in Python that is fully
documented and tested. 
