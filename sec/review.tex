\section{Literature review}\label{sec:review}

This section comprises what could be considered a classical literature review:
an analysis of selected articles and publications that constitute the state and
progress of each research topic.


\subsection{Clustering}\label{subsec:clustering}

Clustering, or \emph{cluster analysis}, is a generic term used to describe a
number of techniques whose objective is to partition some dataset into parts
(clusters) according to some distance (similarity) measure. The generated
partition should be such that the members of one cluster are more similar to one
another than the rest of the data~\cite{Everitt2011}. Often lumped in with the
task of classification, clustering is distinct in that it reveals the underlying
structure of a dataset by only using knowledge that is fixed from the outset.
The alternative being a method with access to some ground truth to refer to,
such as a labelling.

Clustering originated in the social sciences around the turn of the
20\textsuperscript{th} century in a similar manner to several other (now
ubiquitous) statistical tools, including hypothesis testing and \(p\)-values,
correlation, and factor analysis. Early work on cluster analysis, such as Driver
and Kroeber~\cite{Driver1932} and Cattell~\cite{Cattell1943}, focused on its
applications in attempting to identify traits which led to cultural and
psychological differences between groups of people, as opposed to studying the
methods used to identify any partitions.

The focus of clustering research shifted to the statistical nature of the
methods following work by Sokal~\cite{Sokal1966} (later revised with
Sneath~\cite{Sneath1973}) on clustering under the name \emph{numerical
taxonomy}. These works were highly influential and led to an abundance of
research into clustering methods, including~\cite{Diday1976}
and~\cite{Hartigan1975} which each formalised the principles of clustering as a
part of statistics. Furthermore, this period witnessed the advent of seminal
work on clustering algorithms such as \(k\)-means~\cite{Hartigan1979} and
hierarchical clustering~\cite{Defays1977,Sibson1973} that are still considered
fundamental methods.

In tandem with this shift was the rise in the use of software to implement and
compute algorithms (including clustering), and so electronic data became
essential. As the size and complexity of research data increased, so did the
span of the field that is now referred to as \emph{machine learning}. Machine
learning is a loose term to describe how a computer (machine) may `learn' from a
potentially large source of data without following explicit instructions. Owing
in part to this vague definition, machine learning comprises a great deal of
techniques that were borne out of statistics, including regression,
classification and clustering, despite having lost its connection with the
statistics that underpin it, at least colloquially. In particular, clustering
falls comfortably into the category of \emph{unsupervised learning} techniques
as a method that relies solely on observed data and some a priori
knowledge such as a set of parameters~\cite{Dayan1999}.

The remainder of this section offers a brief summary of the literature around
the three principle types of clustering algorithm: those belonging to the
\(k\)-means paradigm, hierarchical connectivity models, and, finally,
density-based algorithms.

\subsubsection{The \(k\)-means paradigm}\label{subsubsec:kmeans}

The concept of \(k\)-means clustering is straightforward: partition a
real-valued dataset into \(k \in \mathbb N\) homogeneous parts (clusters), where
each data point is assigned to the cluster to which the point is closest. The
distance from a point to a cluster is defined as the distance from the point to
the mean of the elements in that cluster, also referred to as the cluster centre
or centroid; this alternative name gives another name to the paradigm,
\emph{centroid-based clustering}. Typically, the distance measure used is the
Euclidean distance, with the assumption that several preprocessing techniques
are used to mitigate any scaling discrepancies in the attributes of the data.

Although \(k\)-means clustering has independently been discovered a number of
times since as early as the 1950s (comprehensive histories on the subject can be
found in~\cite{Bock2007,Jain2010}), its formulation is commonly attributed
to~\cite{Hartigan1979}. Given its lengthy standing, there are a multitude of
algorithms that perform \(k\)-means clustering. However, the most commonly used
is Lloyd's algorithm~\cite{Lloyd1982}. This procedure is remarkably
uncomplicated in its statement, and has now become so synonymous with
\(k\)-means clustering that it is referred to as `the \(k\)-means algorithm'. To
summarise it in a sentence, the algorithm iteratively partitions the numerical
data into \(k\) parts, reassigning data points and adjusting cluster means until
no point moves cluster on a full cycle of the dataset. The algorithm has proven
popular due to this simplicity. Furthermore, it is scalable and can be
parallelised~\cite{Bahmani2012}, and its implementations are
concise~\cite{Olafsson2008,Wu2009}.

However, centroid-based clustering is not without its drawbacks. The most
notable of these is its dependency on being presented with a particular type of
cluster. Namely, the true clusters should be isotropic, convex and
well-separated. The reason these conditions exist, particularly for \(k\)-means,
is because the \(k\)-means (Lloyd's) algorithm is a heuristic for identifying
the centroidal Voronoi tessellation of a dataset~\cite{Du2006}. Hence, the true
objective of the algorithm is to divide the search space into \(k\) equally
weighted parts using the dataset as a sample, rather than focusing on dividing
the dataset itself.

In the time since the popularisation of Lloyd's algorithm, research into
centroid-based clustering has been plentiful, the body of which can be broadly
split into four parts: utilising other measures for the centre of a cluster,
improving the initial choice of centroids, implementing new features to
overcome its particularities about clusters, and selecting a suitable choice for
\(k\).

A cluster centroid is taken as a purely geometric object, and is measured using
a distance metric and a measure of central tendency. Using the Euclidean mean as
the centre of a cluster is a sensible choice when clustering continuous,
normalised data. However, if the data is not scalable (if it is categorical, for
instance) then this approach is not suitable. Further, the Euclidean distance is
linear, leading \(k\)-means to be prone to the effects of outliers in the data.
As such, numerous extensions to Lloyd's algorithm exist in the literature that
are designed to be more robust to these limitations. For continuous (or at least
ordered) data, \(k\)-medians and \(k\)-medoids exist. The former uses the median
value of each attribute in a cluster to form the
centre~\cite{Arya2001,Bradley1997}, mitigating the effect of outliers.

Meanwhile, \(k\)-medoids considers the centre of a cluster to be the data point
which is closest to all other points in the cluster~\cite{Kaufman1987}.
Typically, \(k\)-medoids makes use of the Manhattan distance, but is generic in
its schema. If Euclidean distance is used, then \(k\)-medoids is identical to
\(k\)-means, with the restriction that the centre of the cluster is an actual
point in that cluster. Specifying that the centre be a real point is seen as the
primary benefit of \(k\)-medoids, and continues to be actively
studied~\cite{Schubert2019,Ushakov2021}. There are cases where a virtual cluster
centre is not sufficient, such as facility allocation~\cite{Chen2016,Wang2020}
and the clustering of gene sequences~\cite{Johnson2018}.

When considering a categorical or mixed-type dataset, none of the aforementioned
clustering techniques will work as they were intended because the sense of order
does not exist across the entire attribute space. While it is possible to use
\(k\)-means in these situations, by converting any categorical attributes into
pseudo-numerical (binary) attributes by use of dummy coding, this is not
recommended. One potential failing is the inflated effect of categorical
variables with many distinct values in any distance calculation, as these will
produce more binary variables. Another is that by introducing potentially many
binary variables, the dimensionality of the dataset increases substantially, and
so the points in the higher-dimension form of the data become intrinsically
further from one another, thus loosening any sense of cluster homogeneity.

Therefore, another approach should be taken that handles the data directly. The
\(k\)-modes and \(k\)-prototypes algorithms (introduced in~\cite{Huang1998}) are
capable of handling categorical and mixed-type data, respectively. The
\(k\)-modes algorithm is an extension to Lloyd's algorithm, like \(k\)-medians
and \(k\)-medoids, where the cluster centre is defined as a point whose
attribute values are most frequent among those of the points in a
cluster.~\cite{Huang1998} provides a so-called `matching dissimilarity' measure
to expedite the centre calculations, making \(k\)-modes scalable and
computationally efficient~\cite{Madhuri2014}. \(k\)-prototypes is essentially a
blended form of \(k\)-modes and \(k\)-means where each method is applied to the
categorical and numeric attributes at each iteration, before their respective
costs are combined linearly according to some parameter, \(\gamma\). Work into
\(k\)-modes clustering is largely formed around the initial choice of centroids,
as in~\cite{Cao2009,Jiang2016,Khan2013,Khan2007,Taoying2013}, and improving on
the original distance measure.~\cite{Cao2012} and~\cite{Ng2007} offer two
popular, novel dissimilarity measures for \(k\)-modes clustering that improve on
the basic measure by taking into account the relative frequency of the values of
each attribute.~\cite{Ng2007} focuses locally to the cluster at hand,
while~\cite{Cao2012} considers their frequencies from a universal perspective.

\cite{Sung1998}~demonstrates how the Mahalonobis distance can be used to
circumvent the dependency on spherical clusters by \(k\)-means. The Mahalonobis
distance is a straightforward, outlier-detecting distance measure that is
dependent on the eigenvalues of the covariance matrix derived from a
dataset~\cite{Mahalonobis1936}; a potential downside to this measure is the
singularity of that matrix, which is particularly likely in high-dimensional
data.

\subsection{Healthcare modelling}

Healthcare modelling is a broad term that encompasses a plethora of techniques
from a number of disciplines such as financial modelling and forecasting, and
operational problems like vehicle routing or staff rostering. This review
focuses on these operational pursuits, and particularly those that are concerned
with patients directly. The decision to narrow the literature in this way is
both practical and conscientious. Practical by reducing the span of literature
on `healthcare modelling' to something less cumbersome, and conscientious as it
allows this review to make a small contribution to the research wing of the
progressively more commonplace concept of patient-centred care.

This form of healthcare, formally defined in~\cite{Robinson2008}, demands that
the perspective of a healthcare system should align itself with its patients'
needs and lived experiences. The alternative to patient-centred care would be a
system in which patients are treated in an exact but vague way, according to
only the needs of the system, say. Some form of patient-centred care has been
adopted in healthcare systems around the world including both state-funded and
private systems~\cite{DoH2010,Dewi2013,Luxford2011}. Unsurprisingly, its
application has been commended by patients, advocates and practitioners alike
for improving condition-specific
populations~\cite{Foster2019,Gambling2010,Gondek2016,Tsianakas2012} and more
broadly~\cite{IAPO2012,Richards2015,Santana2019}.

\subsubsection{Segmentation analysis}

Segmentation analysis allows for the targeted analysis of otherwise
heterogeneous datasets and encompasses several techniques from operational
research, statistics and machine learning. One of the most desirable qualities
of this kind of analysis is the ability to glean and communicate simplified
summaries of patient needs to stakeholders within a healthcare
system~\cite{Vuik2016b, Yoon2020}. For instance, clinical profiling often forms
part of the broader analysis where each segment is summarised in a phrase or
infographic~\cite{Vuik2016a,Yan2019}.

The survey identified three commonplace groups of patient characteristics used
to segment a patient population: system utilisation metrics; clinical
attributes; and the pathway. The last is not used to segment the patients
directly, but instead groups patients' movements through a healthcare system;
this is typically done using a technique known as process
mining.~\cite{Arnolds2018}~and~\cite{Delias2015} demonstrate how this technique
can be used to improve the efficiency of a hospital system as opposed to
tackling the more relevant issue of patient-centred care. The remaining
characteristics can be segmented in a variety of ways, but recent works tend to
favour unsupervised methods --- typically latent class analysis (LCA) or
clustering~\cite{Yan2018}.

LCA is a statistical, model-based method used to identify groups (called latent
classes) in data by relating its observations to some unobserved (latent),
categorical attribute. This attribute has multiple possible categories, each
corresponding to a latent class. The discovered relations enable the
observations to be separated into latent classes according to their maximum
likelihood class membership~\cite{Hagenaars2002,Lazarsfeld1968}. This method has
proved useful in the study of comorbidity patterns as
in~\cite{Kuwornu2014,Larsen2017} where combinations of demographic and clinical
attributes are related to various subgroups of chronic diseases.

Similarly to LCA, clustering identifies groups (clusters) in data to produce
labels for its instances. However, as discussed in
Section~\ref{subsec:clustering}, clustering includes a wide variety of methods
where the common theme is to maximise homogeneity within, and heterogeneity
between, each cluster. Of those methods, \(k\)-means clustering is perhaps the
most widely used in healthcare; this is likely due to its simplicity and
scalability. In addition to \(k\)-means, hierarchical clustering methods can be
useful if a suitable number of parts cannot be found initially~\cite{Vuik2016a}.
Moreover, supervised hierarchical segmentation methods such as classification
and regression trees (as in~\cite{Harper2006}) have been used where an existing,
well-defined, label is of particular significance.

\subsubsection{Queuing models}

Since the seminal works by Erlang~\cite{Erlang1917,Erlang1920} established the
core concepts of queuing theory, the application of queues and queuing networks
to real services has become abundant, including the healthcare service. By
applying these models to healthcare settings, many aspects of the underlying
system can be studied. A common area of study in healthcare settings is of
service capacity.~\cite{McClain1976} is an early example of such work where
acute bed capacity was determined using hospital occupancy data. Meanwhile, more
modern works such as~\cite{Palvannan2012,Pinto2014} consider more extensive
sources of data to build their queuing models.  Moreover, the output of a model
is catered more towards being actionable --- as is the prerogative of
operational research. For instance,~\cite{Pinto2014} devises new categorisations
for both hospital beds and arrivals that are informed by the queuing model. A
further example is~\cite{Komashie2015} where queuing models are used to measure
and understand satisfaction among patients and staff.

In addition to these theoretic models, healthcare queuing research has expanded
to include computer simulation models. The simulation of queues, or networks
thereof, have the benefit of adeptly capturing the stochastic nuances of
hospital systems over their theoretic counterparts. Example areas include the
construction and simulation of Markov processes via process
mining~\cite{Arnolds2018,Rebuge2012}, and patient flow~\cite{Bhattacharjee2014}.
Regardless of the advantages of simulation models, a prerequisite is reliable
software with which to construct those simulations. A common approach to
building simulation models of queues is to use a graphical user interface such
as Simul8. These tools have the benefits of being highly visual, making them
attractive to organisations looking to implement queuing models without
necessary technical expertise, including the NHS.~\cite{Brailsford2013}
discusses the issues around operational research and simulation being taken up
in the NHS despite the availability of intuitive software packages like Simul8.
However, they do not address a core principle of good simulation work:
reproducibility. The ability to reliably reproduce a set of results is of great
importance to scientific research but remains an issue in simulation research
generally~\cite{Fitzpatrick2019}. When considering issues with reproducibility
in scientific computing (simulation included), the source of any concerns is
often with the software used~\cite{Ivie2018}. Using well-developed, open-source
software can alleviate issues around reproducibility and reliability as how they
are used involve less uncertainty and require more rigour than ‘drag-and-drop’
software. One example of such a piece of software is Ciw~\cite{Palmer2019}. Ciw
is a discrete event simulation library written in Python that is fully
documented and tested. 
